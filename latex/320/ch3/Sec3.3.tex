\documentclass[letterpaper,12pt]{article}

\usepackage{threeparttable}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1.25in,rmargin=1.25in}
\usepackage[format=hang,font=normalsize,labelfont=bf]{caption}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{array}
\usepackage{delarray}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{lscape}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{float,color}
\usepackage[pdftex]{graphicx}
\usepackage{mathrsfs}  
\usepackage{pdfsync}
\usepackage{verbatim}
\usepackage{placeins} \usepackage{geometry}
\usepackage{pdflscape}
\synctex=1
\usepackage{hyperref}
\hypersetup{colorlinks,linkcolor=red,urlcolor=blue,citecolor=red}
\usepackage{bm}
\usepackage{amssymb}


\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}{Definition} % Number definitions on their own
\newtheorem{derivation}{Derivation} % Number derivations on their own
\newtheorem{example}[theorem]{Example}
\newtheorem*{exercise}{Exercise} % Number exercises on their own
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}{Proposition} % Number propositions on their own
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\bibliographystyle{aer}
\newcommand\ve{\varepsilon}
\renewcommand\theenumi{\roman{enumi}}

\title{Sec 3.3 \\Math 320}
\author{Rex McArthur}

\begin{document}
\maketitle
\exercise{3.14}\\
(i)\\
\begin{align*}
    f_X(\alpha) = \begin{cases} \frac{1}{2} &\mbox{if } \alpha = a,b,c\\
        \frac{1}{6} & \mbox{if } \alpha = d \\
        \frac{1}{3} & \mbox{if } \alpha = e,f \\
        0 & \mbox{elsewhere }\\
    \end{cases}
\end{align*}

(ii)\\
\begin{align*}
    \frac{1}{2} + \frac{1}{6}\cdot 2 + \frac{1}{3} \cdot 3.5 = 2
\end{align*}

(iii)\\
\begin{align*}
    \text{var}(x) &= (1-2)^2 \cdot \frac{1}{2} + (0)^2 \cdot \frac{1}{6} + (1.5)^2 \cdot \frac{1}{3} = \frac{5}{4}
\end{align*}

\exercise{3.15}\\
Let $X$ be a discrete set, and have probability distribution $p(x=i) = g_X$
Suppose $h$ is a continuous function s.t. $h:\mathbb{R} \rightarrow \mathbb{R}$, and $\exists \epsilon > 0$ between each point in $X$ where there are countable number of points.
Thus, because $h$ is continuous, $\exists \epsilon' >0$ between each point. Let $A = \{\epsilon '\}_{i\in I}$.
Choosing the inf$(A)$ will yield an $\epsilon'$, such that for every point $p$, 
$\exists E(p,\frac{\epsilon'}{2})$, such that the neighborhood $E$ contains no other points in $X$. Thus, $h(X)$ is a random variable, and by LOTUS,
\begin{align*}
    E[h(X)] &= \sum_{i \in I}h(i)p(x = i) = \sum_{i \in I} h(i)g_X(i)
\end{align*}

\exercise{3.16}\\
\begin{align*}
    \text{var}[X] & = E((X-\mu)^2) = E(X^2 - 2X\mu + \mu^2) \\
    & = E(X^2) - 2\mu E(X) + E(\mu^2)\\
    & = E(X^2) - \mu^2
\end{align*}

\exercise{3.17}\\
\begin{align*}
    \text{var}(\alpha x + \beta y) & = E (\alpha x + \beta y)^2 - (E(\alpha x + \beta y) )^2 \\
    & = E(\alpha^2 x^2 + \alpha x \beta y + \alpha \beta xy + \beta^2 y^2) - (\alpha E(x) + \beta E(Y))^2 \\
    & = \alpha^2 E(x^2) + 2\alpha \beta E(xy) + \beta ^2 E(y^2) - \alpha E(x)^2 - 2 \alpha \beta E(x)E(y) - \beta ^2 E(y)^2 \\
    & = \alpha ^2 (E(x^2)-E(x)^2) + 2\alpha \beta (E(xy)-E(x)E(y)) + \beta^2(E(y^2)-E(y)^2) \\
    & = \alpha^2 \text{var}[x] + 2\alpha \beta(E(xy)-E(x)E(y))+\beta^2 \text{var}[y]
\end{align*}
As desired, and if $x,y$ are independent, we know that $E(xy)-E(x)E(y)= 0$, and thus
\[
    \text{var}(\alpha x + \beta y) = \alpha^2 \text{var}[x] +\beta^2 \text{var}[y]
\]


\exercise{3.18}\\
Note, by LOTUS $E\big(\frac{1}{X+1}\big)$ is a random variable.
\begin{align*}
    E(\frac{1}{X+1}) & = \sum_{i=0}^n \frac{1}{i+1} \binom(n,i) p^i (1-p)^{n-i} \\
    & = \sum_{i=0}^n \frac{n!}{(i+1)!(n-i)!}  p^i (1-p)^{n-i} \\
    & = \sum_{i=1}^{n+1} \frac{n!}{i!(n-i+1)!} p^{i-1} (1-p)^{n-i+1}\\
    & = \frac{1}{p(n+1)} \sum_{i=1}^{n+1} \frac{(n+1)!}{i!(n+1-i)!} p^{i} (1-p)^{n+1-i} \\
    & = \frac{1}{p(n+1)} \sum_{i=1}^{n+1} \binom(n+1,i) p^{i} (1-p)^{n+1-i} \\
    & = \frac{1}{p(n+1)} \big(\sum_{i=0}^{n+1} \binom(n+1,i) p^{i} (1-p)^{n+1-i}-(1-p)^{n+1}\big) \\
    & = \frac{1}{p(n+1)}\cdot (1-(1-p)^{n+1}) \\
    & = \frac{1-(1-p)^{n+1}}{p(n+1)} 
\end{align*}

\exercise{3.19}\\
\begin{align*}
    E(X) &= \sum_{x \in X} x \cdot P(X=x) \\
    & = \sum_{i \in I} x \sum{x \in X} \frac{x P(X = x) \bigcap B_i) P(B_i)}{P(B_i)}\\
    & = \sum_i \sum_x x P(X=x|B_i)P(B_i) \\
    & = \sum_i E(X|B_i)P(B_i)
\end{align*}




\end{document}

